import pandas as pd
import numpy as np
import optuna
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° í‘œì¤€í™”
X_train = pd.read_csv('X_train.csv')
X_test = pd.read_csv('X_test.csv')
y_train = pd.read_csv('y_train.csv').values.ravel()
y_test = pd.read_csv('y_test.csv').values.ravel()

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# sleep_quality_score ì¶œë ¥
print('sleep_quality_score ìµœì†Œê°’:', y_train.min())
print('sleep_quality_score ìµœëŒ€ê°’:', y_train.max())
print(pd.Series(y_train).describe())

# âœ… Optuna ëª©ì  í•¨ìˆ˜ (5-fold CV ì ìš©)
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 5, 40),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),
        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
    }
    model = RandomForestRegressor(random_state=42, **params, n_jobs=-1)
    scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2', n_jobs=-1)
    return np.mean(scores)

# âœ… 10ê°œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥ ì½œë°± í•¨ìˆ˜
def print_every_10_trials(study, trial):
    if (trial.number + 1) % 10 == 0:
        print(f"\n===== {trial.number + 1}ê°œ trial ì™„ë£Œ! =====")
        print(f"Best value: {study.best_value}")
        print(f"Best params: {study.best_params}")

# âœ… Optuna Study ì‹¤í–‰ 
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42),
    pruner=optuna.pruners.MedianPruner()
)
study.optimize(
    objective,
    n_trials=100,
    n_jobs=12,
    callbacks=[print_every_10_trials]
)

# âœ… ìµœì  ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
best_params = study.best_params
print("\nâœ… Best Parameters:", best_params)

final_model = RandomForestRegressor(random_state=42, **best_params, n_jobs=-1)
final_model.fit(X_train_scaled, y_train)
y_pred = final_model.predict(X_test_scaled)

# âœ… ì˜ˆì¸¡ê°’ ë³´ì • (0~100 ë²”ìœ„ë¡œ clip)
y_pred = np.clip(y_pred, 0, 100)

# âœ… ì„±ëŠ¥ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("\nâœ… ìµœì¢… ëª¨ë¸ ì„±ëŠ¥")
print(f"RÂ² Score : {r2:.4f}")
print(f"RMSE     : {rmse:.4f}")
print(f"MAE      : {mae:.4f}")

# âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
pd.DataFrame({"Actual": y_test, "Predicted": y_pred}).to_csv("rf_predictions.csv", index=False)
print("ğŸ“„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: rf_predictions.csv")

# âœ… ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
importances = final_model.feature_importances_
feature_names = X_train.columns
indices = np.argsort(importances)[::-1][:20]

plt.figure(figsize=(12, 6))
plt.bar(range(len(indices)), importances[indices])
plt.xticks(range(len(indices)), [feature_names[i] for i in indices], rotation=45, ha='right')
plt.title("Top 20 Feature Importances (Optuna Tuned RF)")
plt.tight_layout()
plt.show()

# âœ… ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.6, edgecolors='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', lw=2)
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title("Actual vs Predicted (Optuna RF)")
plt.grid(True)
plt.tight_layout()
plt.show()

# âœ… ì˜ˆì¸¡ ê²°ê³¼ 3ê°œ êµ¬ê°„ ë¶„ë¥˜ (bad/mid/good)
bins = np.quantile(y_test, [0, 0.33, 0.66, 1.0])
y_test_cls = np.digitize(y_test, bins=bins[1:], right=False)
y_pred_cls = np.digitize(y_pred, bins=bins[1:], right=False)

from sklearn.metrics import confusion_matrix, classification_report

conf_matrix = confusion_matrix(y_test_cls, y_pred_cls)
report = classification_report(y_test_cls, y_pred_cls, target_names=['bad', 'mid', 'good'])

print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(report)

plt.figure(figsize=(5, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred_bad', 'Pred_mid', 'Pred_good'],
            yticklabels=['True_bad', 'True_mid', 'True_good'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (3-class by quantile)')
plt.tight_layout()
plt.show()
